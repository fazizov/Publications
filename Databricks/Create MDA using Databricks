# Databricks notebook source
# blobaccname=dbutils.secrets.get(scope="AiKvScope2", key="blobAccName")
# blobkey=dbutils.secrets.get(scope="AiKvScope2", key="blobkey")
# dbutils.fs.mount(source = "wasbs://mda@{0}.blob.core.windows.net".format(blobaccname),mount_point = "/mnt/mda/",
#     extra_configs = {"fs.azure.account.key.{0}.blob.core.windows.net".format(blobaccname):"{0}".format(blobkey)})

# COMMAND ----------

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.types import StructType

# COMMAND ----------

# MAGIC %md ####Create metadata tables

# COMMAND ----------

def createMetaTables():
  spark.sql("CREATE DATABASE IF NOT EXISTS BRONZE")
  spark.sql("CREATE DATABASE IF NOT EXISTS SILVER")
  spark.sql("CREATE DATABASE IF NOT EXISTS GOLD")
  spark.sql("CREATE DATABASE IF NOT EXISTS ETL")
  spark.sql("DROP TABLE IF EXISTS ETL.TableNames")
  sqlStr="CREATE  TABLE ETL.TableNames (\
  TableName_GLD string,\
  TableName_SLV string,\
  TableName_BRZ string,\
  SourcePath string,\
  TableType string,\
  ChangeTrackingType int,\
  WatermarkDate timestamp,\
  LastId int)\
  USING DELTA"
  spark.sql(sqlStr)
  #TableFields
  spark.sql("DROP TABLE IF EXISTS ETL.TableFields")
  sqlStr="CREATE TABLE ETL.TableFields (\
  SchemaName string,\
  TableName string,\
  ColumnName string,\
  SourceSchemaName string,\
  SourceTableName string,\
  SourceColumnName string,\
  LookupColumnName string,\
  ColumnOrder int,\
  ColumnType string,\
  IsSurrogateKeyFlg boolean,\
  IsBusinessKeyFlg boolean,\
  IsTimestampFlg boolean,\
  IsPartitionKeyFlg boolean,\
  IsSCDFlg boolean,\
  IsDateKey boolean,\
  IsSnapshotKey boolean )\
  USING DELTA"
  spark.sql(sqlStr)
  #Logs
  spark.sql("drop table if exists ETL.Logs")
  sqlStr="CREATE  TABLE ETL.Logs (\
  SchemaName string,\
  TableName string,\
  Operation string,\
  InsertedRows int,\
  UpdatedRows int,\
  DeletedRows int,\
  UpdateDateTime timestamp)\
  USING DELTA"
  spark.sql(sqlStr)
  print ("Metadata tables have been created")
  return

# COMMAND ----------

# MAGIC %md ####Populate metadata tables

# COMMAND ----------

def populateTableNames():
  spark.sql("truncate table ETL.TableNames")
  spark.sql("drop table if exists ETL.TableNames_STG")
  sqlStr="create  table ETL.TableNames_STG (\
  TableName_GLD string,\
  TableName_SLV string,\
  TableName_BRZ string,\
  SourcePath string,\
  TableType string,\
  ChangeTrackingType int,\
  WatermarkDate string,\
  LastId int\
  )\
  USING CSV\
  OPTIONS (header 'true')\
  LOCATION '/mnt/mda/source/Metadata/Metadata_Tables.csv'"
  spark.sql(sqlStr)      
  spark.sql("INSERT INTO ETL.TableNames SELECT TableName_GLD,TableName_SLV,TableName_BRZ,SourcePath,TableType,ChangeTrackingType,\
    CAST(WatermarkDate AS timestamp),LastId FROM ETL.TableNames_STG ")
  print ("Table names metedata has been populated")
  return

def populateTableFields():
  spark.sql("truncate table ETL.TableFields")
  spark.sql("drop table if exists ETL.TableFields_STG")
  sqlStr="create  table ETL.TableFields_STG (\
  SchemaName string,\
  TableName string,\
  ColumnName string,\
  SourceSchemaName string,\
  SourceTableName string,\
  SourceColumnName string,\
  LookupColumnName string,\
  ColumnOrder int,\
  ColumnType string,\
  IsSurrogateKeyFlg int,\
  IsBusinessKeyFlg int,\
  IsTimestampFlg int,\
  IsPartitionKeyFlg int,\
  IsSCDFlg int,\
  IsDateKey int,\
  IsSnapshotKey int)\
  USING CSV\
  OPTIONS (header 'true')\
  LOCATION '/mnt/mda/source/Metadata/Metadata_Columns.csv'"
  spark.sql(sqlStr)           
  spark.sql("INSERT INTO ETL.TableFields  SELECT SchemaName,TableName,ColumnName,SourceSchemaName,SourceTableName,SourceColumnName,LookupColumnName,ColumnOrder,ColumnType,\
  IsSurrogateKeyFlg,IsBusinessKeyFlg,IsTimestampFlg,IsPartitionKeyFlg,IsSCDFlg,IsDateKey,IsSnapshotKey FROM ETL.TableFields_STG WHERE TableName IS NOT NULL ORDER BY TableName,ColumnOrder")
  #Add Sourcefilename metadata
  spark.sql("INSERT INTO ETL.TableFields  SELECT 'BRONZE',TableName_BRZ,'SourceFileName',Null,Null,Null,Null,999,'STRING',False,False,False,False,False,False,False \
  FROM ETL.TableNames WHERE TableName_BRZ IS NOT NULL  ")
  print ("Column names metadata has been populated")
  return

def InitializeMetadata():
  createMetaTables()
  populateTableNames()
  populateTableFields()
  return

# COMMAND ----------

# MAGIC %md ####Create DWH tables

# COMMAND ----------

def getTableCreateCmd(tableName,schemaName):
   partitionKey=getKeyFieldName(tableName,"IsPartitionKeyFlg")
#    print (tableName,tableSchema) 
   dropCmdStr="DROP TABLE IF EXISTS {1}.{0}".format(tableName,schemaName) 
   createCmdStr="CREATE TABLE {1}.{0} ( ".format(tableName,schemaName)
   dfCols=spark.sql("SELECT ColumnName,ColumnType FROM ETL.TableFields WHERE TableName='{0}' ORDER BY ColumnOrder".format(tableName))
   if dfCols.count()>0:
     rdd=dfCols.rdd.map(lambda x:','+x['ColumnName']+'  '+x['ColumnType'])
     createCmdStr+=rdd.reduce(lambda x,y:x+y).replace(",","",1)
     createCmdStr+=") USING DELTA "
     if partitionKey is not None:
       createCmdStr+=" PARTITIONED BY ({0})".format(partitionKey)
   else:
      createCmdStr=""
   return (dropCmdStr,createCmdStr)

def populateDummyRecords():
   print ("Populating dimension tables with dummy records") 
#    tabLst=dfTabs.collect()
   tabLst= getQueryResultsAsList("SELECT TableName_SLV FROM ETL.TableNames WHERE TableName_SLV IS NOT NULL AND TableType='D'")
   for row in tabLst:
     tableName=row['TableName_SLV']
     rows=getQueryResultsAsList("SELECT ColumnName,IsSurrogateKeyFlg FROM ETL.TableFields WHERE TableName='{0}' ORDER BY ColumnOrder".format(tableName))
     valuesStrMiss=""
     valuesStrUnkn=""
     for col in rows:
        if col['IsSurrogateKeyFlg']==True:
          valuesStrMiss+=",-1"
          valuesStrUnkn+=",-2"
        elif col['ColumnName']=="CurrentFlg":
          valuesStrMiss+=",1"
          valuesStrUnkn+=",1"
        elif col['ColumnName']=="ValidFromDate":
          valuesStrMiss+=",CAST('1900-01-01' as timestamp)"
          valuesStrUnkn+=",CAST('1900-01-01' as timestamp)"       
        elif col['ColumnName']=="ValidToDate":
          valuesStrMiss+=",CAST('9999-12-31' as timestamp)"
          valuesStrUnkn+=",CAST('9999-12-31' as timestamp)"       
        else:  
          valuesStrMiss+=",Null"  
          valuesStrUnkn+=",Null"  
     valuesStrMiss=valuesStrMiss.replace(',','',1)
     valuesStrUnkn=valuesStrUnkn.replace(',','',1)
     insertDummyRowsCmd="INSERT INTO SILVER.{0} VALUES ({1}),({2})".format(tableName,valuesStrMiss,valuesStrUnkn)
#      print(insertDummyRowsCmd)
     spark.sql(insertDummyRowsCmd) 
   return

def createDWHTables(SchemaName):
  if SchemaName=="BRONZE":
    TableColNm="TableName_BRZ"
  else:
    TableColNm="TableName_SLV"
  tabLst=getQueryResultsAsList("SELECT {0} FROM ETL.TableNames WHERE {0} IS NOT NULL".format(TableColNm))
  for row in tabLst:
#     print ("Table {0}".format(row[TableColNm]))
    dropBrzStr,createBrzStr=getTableCreateCmd(row[TableColNm],SchemaName)
#     print (createBrzStr)
    spark.sql(dropBrzStr)
    spark.sql(createBrzStr)
    print("Table {} has been created".format(row[TableColNm]))
  populateDummyRecords()  
  return        

# COMMAND ----------

# MAGIC %md ####Utility functions

# COMMAND ----------

def getQueryResultsAsList(sqlCmd):
  df=spark.sql(sqlCmd)
  if df.count()>0:
    lst=df.collect()
  else:
    lst=None
  return lst

def getWatermarkData(TableName):
  sqlStrWtrm=("SELECT WatermarkDate,LastId FROM ETL.TableNames where TableName_SLV='{0}'").format(TableName)
  dfWM=spark.sql(sqlStrWtrm)
  return (dfWM.first())  

def updateWatermarkData(TableName,TableType):
  if TableType=='D':
    surrKeyName=getKeyFieldName(TableName,"IsSurrogateKeyFlg") 
    sqlMaxVal="SELECT MAX({1}) AS MaxID FROM SILVER.{0} ".format(TableName,surrKeyName)
    maxID=spark.sql(sqlMaxVal).first()['MaxID']
#   print(maxID)
    if maxID is not None:
      sqlStrWtrm=("UPDATE ETL.TableNames SET WatermarkDate =current_timestamp(),LastId={0} WHERE TableName_SLV='{1}'").format(maxID,TableName)
      dfWM=spark.sql(sqlStrWtrm)
    else:
      print ("Table {0} has no rows".format(TableName))  
#     print (sqlStrWtrm)
  else:
      sqlStrWtrm=("UPDATE ETL.TableNames SET WatermarkDate =current_timestamp() WHERE TableName_SLV='{0}'").format(TableName)
      dfWM=spark.sql(sqlStrWtrm)
  return 

def getKeyFieldName(tableName,keyType):
    df=spark.sql("SELECT ColumnName FROM ETL.TableFields WHERE TableName='{0}' and {1}=true".format(tableName,keyType))
    if df.count()>0:
      keyFieldName=df.first()['ColumnName']
    else:
      keyFieldName=None
    return keyFieldName
  
def getKeyFieldsListStr(schemaName,tableName,keyType):
    df=spark.sql("SELECT ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' and {2}=true".format(schemaName,tableName,keyType))
    if df.count()>0:
      rddMap=df.rdd.map(lambda x:','+x['ColumnName'])
      colLstStr=rddMap.reduce(lambda x,y:x+y)
    else:
      colLstStr=""
    return colLstStr  
  
def getFieldList(SchemaName,TableName,tableAlias="",targetColumnsOnlyFlg=True):
  colList=getQueryResultsAsList("SELECT ColumnName,SourceColumnName,SourceTableName,LookupColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' ORDER BY ColumnOrder".format(SchemaName,TableName))
  colLstStr=""
  aliasNum=1
  for row in colList:
    if targetColumnsOnlyFlg==True:
      colLstStr+=",{0}{1}".format(tableAlias,row['ColumnName'])
    else:
      if row['SourceColumnName'] is not None:
#         colLstStr+=",{0}{1}".format(tableAlias,row['ColumnName'])
#       else:
        if row['LookupColumnName'] is not None:
          colLstStr+=",COALESCE(LKP{0}.{1},-1) AS {2}".format(aliasNum,row['SourceColumnName'],row['ColumnName'])
#           colLstStr+=",CASE WHEN LKP{0}.{1} IS NOT NULL THEN LKP{0}.{1} ELSE -1 END AS {2}".format(aliasNum,row['SourceColumnName'],row['ColumnName'])      
          aliasNum+=1
        else:  
          colLstStr+=",{0}{1} AS {2}".format(tableAlias,row['SourceColumnName'],row['ColumnName'])
  colLstStr=colLstStr.replace(",","",1)
  return colLstStr

def getJoinConditions(schemaName,tableName,sourceAlias,targetAlias):
  sqlCmd="SELECT ColumnName,SourceColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' and IsBusinessKeyFlg=true".format(schemaName,tableName)
  df=spark.sql(sqlCmd)
  joinCond=''
  if df.count()>0:
    rddMap=df.rdd.map(lambda x:" AND {0}.{1}={2}.{3} ".format(sourceAlias,x['SourceColumnName'],targetAlias,x['ColumnName']))
    joinCond=rddMap.reduce(lambda x,y:x+y).replace("AND","",1)  
  return joinCond

def getLkpJoinList(SchemaName,TableName):
  colList=getQueryResultsAsList("SELECT ColumnName,SourceSchemaName,SourceTableName,SourceColumnName,LookupColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' \
   AND LookupColumnName IS NOT NULL ORDER BY ColumnOrder".format(SchemaName,TableName))
  aliasNum=1
  joinStr= ""
  if colList is not None:
    for row in colList:
      lkpAlias="LKP{0}".format(aliasNum)
      lkpBusinessKeyField=getKeyFieldName(row['SourceTableName'],"IsBusinessKeyFlg")
      joinStr+= " LEFT JOIN {0}.{1} AS {2} ON S.{3}={2}.{4} ".format(row['SourceSchemaName'],row['SourceTableName'],lkpAlias,row['LookupColumnName'],lkpBusinessKeyField)
      if row['SourceSchemaName']=="SILVER":
        joinStr+=" AND {0}.CurrentFlg=1 ".format(lkpAlias)
      aliasNum+=1
  return joinStr

def getBronzeTableName(tableName):
  sqlStrWtrm=("SELECT TableName_BRZ FROM ETL.TableNames where TableName_SLV='{0}'").format(tableName)
  dfWM=spark.sql(sqlStrWtrm)
  return (dfWM.first()['TableName_BRZ']) 

def getRowCounts():
  dfRows=getQueryResultsAsList("SELECT TableName_BRZ,TableName_SLV FROM ETL.TableNames ")
  for row in dfRows:
    if row['TableName_BRZ'] is not None:
      rowCnt_BRZ=spark.table("BRONZE.{0}".format(row['TableName_BRZ'])).count()
#       print ("BRONZE.{0}:{1}".format(row['TableName_BRZ'],rowCnt_BRZ))
    else:
      rowCnt_BRZ=-1  
    if row['TableName_SLV'] is not None:
      rowCnt_SLV=spark.table("SILVER.{0}".format(row['TableName_SLV'])).count()
      print ("Row counts- BRONZE.{0}:{1}, SILVER.{2}:{3}".format(row['TableName_BRZ'],rowCnt_BRZ,row['TableName_SLV'],rowCnt_SLV))
#       print ("SILVER.{0}:{1}".format(row['TableName_SLV'],rowCnt_SLV))
    else:
      rowCnt_SLV=-1
      print ("Row counts- BRONZE.{0}:{1}, No matching SILVER table".format(row['TableName_BRZ'],rowCnt_BRZ))
  return

def resetWatermarkData():
  spark.sql("UPDATE ETL.TableNames SET LastId=1,WatermarkDate='1900-01-01'")
  return

def resetETLLogs():
  spark.sql("TRUNCATE TABLE ETL.Logs")
  return

def cleanDWHTables(schemaName):
  dfRows=getQueryResultsAsList("SELECT TableName_BRZ,TableName_SLV FROM ETL.TableNames ")
  for row in dfRows:
    if schemaName=="SILVER" and row['TableName_SLV'] is not None:
      spark.sql("TRUNCATE TABLE SILVER.{0}".format(row['TableName_SLV']))
      resetWatermarkData()
      print ("Table SILVER.{} has been cleaned".format(row['TableName_SLV']))
    elif schemaName=="BRONZE" :   
      spark.sql("TRUNCATE TABLE BRONZE.{0}".format(row['TableName_BRZ']))  
      print ("Table BRONZE.{} has been cleaned".format(row['TableName_BRZ']))
  return

def updateETLLogs(schemaName,tableName):
  sqlCmd="DESCRIBE HISTORY {0}.{1}".format(schemaName,tableName)
#   print (sqlCmd)
  df=spark.sql(sqlCmd)
  row=df.orderBy(df.version, ascending=False).selectExpr("operation","operationParameters.mode As UpdateMode","operationMetrics.numOutputRows As TotalRows",\
     "operationMetrics.numUpdatedRows As UpdatedRows","operationMetrics.numDeletedRows As DeletedRows","operationMetrics.numTargetRowsUpdated As MergedRows","timestamp").first()
  insCnt=updCnt=delCnt=0
  if row["operation"]=="UPDATE":
    updCnt=row["UpdatedRows"]
  elif row["operation"]=="DELETE":
    delCnt=row["DeletedRows"]
  elif row["operation"]=="MERGE":
    updCnt=row["MergedRows"]
  elif row["operation"]=="WRITE":
    insCnt=row["TotalRows"]
  insertCmd="INSERT INTO ETL.Logs VALUES ('{0}','{1}','{2}',{3},{4},{5},CAST('{6}' as timestamp))".format(schemaName,tableName,row["operation"],insCnt,updCnt,delCnt,row["timestamp"])
  spark.sql(insertCmd)
  return

def getKeyFieldList(tableName,keyType):
    keyFieldList=getQueryResultsAsList("SELECT ColumnName,LookupColumnName FROM ETL.TableFields WHERE TableName='{0}' and {1}=true".format(tableName,keyType))
    return keyFieldList

def populateDimDatesFromFacts():
  print ("Populating date dimensions from the facts")
  dfRows=getQueryResultsAsList("SELECT TableName_BRZ,TableName_SLV FROM ETL.TableNames WHERE TableName_SLV IS NOT NULL")
  selCmd=""
  for row in dfRows:
    tableName_SLV=row['TableName_SLV']
    tableName_BRZ=row['TableName_BRZ']
    dtFldLst=getKeyFieldList(tableName_SLV,"IsDateKey")
    if dtFldLst is not None:
      for colRow in dtFldLst:
        if len(selCmd)>0:
          selCmd+=" UNION "
        selCmd+="SELECT year({0})*10000+month({0})*100+dayofmonth({0}) AS DateKey ,{0} AS Date,year({0}) as CalendarYear,quarter({0}) as CalendarQuarter,\
        month({0}) as CalendarMonth,dayofmonth({0}) CalendarDay,CAST('1900-01-01' AS TIMESTAMP) AS ValidFromDate,CAST('9999-12-31' AS TIMESTAMP) AS ValidToDate,1 AS CurrentFlg FROM BRONZE.{1} ".format(colRow['LookupColumnName'],tableName_BRZ)
  mergeCmd="MERGE INTO SILVER.DimDate AS T USING ({0}) AS S ON T.DateKey=S.DateKey WHEN NOT MATCHED THEN INSERT * ".format(selCmd)
#   print (mergeCmd)
  spark.sql(mergeCmd)
  return

# COMMAND ----------

# MAGIC %md ####BRONZE level ingestions

# COMMAND ----------

def ingestBronzeTables(truncateFirstFlg,printSQLCmdsFlg=False):
#   df=spark.sql("Select TableName_BRZ,SourcePath from ETL.TableNames")
  tbsArr=getQueryResultsAsList("Select TableName_BRZ,SourcePath from ETL.TableNames")
  for tableItem in tbsArr:
      if tableItem.SourcePath is not None:
        sourcePath="{0}{1}.csv".format(tableItem.SourcePath,tableItem.TableName_BRZ)
        df=spark.read.csv(sourcePath,header=True)
        rootFolderStr=tableItem.SourcePath+"|dbfs:"
        df=df.withColumn("SourceFileName",F.regexp_replace(F.input_file_name(),rootFolderStr,""))
#         SourcePath="/mnt/mda/source/|dbfs:"
# df=df.withColumn("SourceFileName",F.regexp_replace(F.input_file_name(),SourcePath,""))
        df.createOrReplaceTempView("TempSource")
        fldLstStr=getFieldList("BRONZE",tableItem.TableName_BRZ,"",True)
        sqlStr="INSERT INTO BRONZE.{0} SELECT {1} FROM TempSource".format(tableItem.TableName_BRZ,fldLstStr)
        print("Ingestion for table {0} is complete".format(tableItem.TableName_BRZ))
        if printSQLCmdsFlg==True:
           print (sqlStr)
        if truncateFirstFlg ==True:
          spark.sql("TRUNCATE TABLE BRONZE.{0}".format(tableItem.TableName_BRZ))
        spark.sql(sqlStr)
        updateETLLogs("BRONZE",tableItem.TableName_BRZ)
#   print(df.count())
  return  

# COMMAND ----------

# MAGIC %md ####SILVER level ingestions

# COMMAND ----------

def getFactInsertSubQuery(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,fldLstFrmSrc,lkpJoinList,busKeyFld,ChangeTrackingType):
    delCmd=""
    if ChangeTrackingType==1:
      insertCmd="SELECT {2} FROM BRONZE.{0} AS S {4} WHERE S.{5}>='{6}'\
       ".format(tableName_BRZ,tableName_SLV,fldLstFrmSrc,srcTrgJoinCond,lkpJoinList,tmspFld,lastUpdDt,srcTrgJoinCond,busKeyFld)
    elif ChangeTrackingType==2:
      delCmd=getFactDelCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt)
      insertCmd="SELECT {2} FROM BRONZE.{0} AS S {4} WHERE S.{5}>='{6}'\
       ".format(tableName_BRZ,tableName_SLV,fldLstFrmSrc,srcTrgJoinCond,lkpJoinList,tmspFld,lastUpdDt,srcTrgJoinCond,busKeyFld)
    print (insertCmd)
#   print("Ingestiion for {0} is complete".format(tableItem.TableName_BRZ))
    return (insertCmd,delCmd)   #Add update statement

def ingestDimTables(tableArr,truncateFirstFlg,initialUploadFlg,printSQLCmdsFlg):
  for tableItem in tableArr:
    sqlUpdateStr,delCmd=getUpdInsCmdsNew(tableItem.TableName_SLV,'D',tableItem.ChangeTrackingType)
    if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE SILVER.{0}".format(tableItem.TableName_SLV))
#      elif initialUploadFlg==False:  
    if printSQLCmdsFlg==True:
      print(sqlUpdateStr)
    spark.sql(sqlUpdateStr)
    updateETLLogs("SILVER",tableItem.TableName_SLV) 
    updateETLLogs("SILVER",tableItem.TableName_SLV) 
    updateWatermarkData(tableItem.TableName_SLV,'D') 
#      print(sqlUpdateStr)
  return  


def getDimMergeCmdNew(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChangeTrackingType,insertSubquery):
  if ChangeTrackingType==1:  
    mrgCmd=getMergeOverwriteCmdNew(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChangeTrackingType,insertSubquery)
  elif ChangeTrackingType==2:
    mrgCmd="MERGE INTO SILVER.{0} AS T USING ({1}) AS S ON {4} \
    WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp(),CurrentFlg=0 \
    WHEN NOT MATCHED THEN INSERT *".format(tableName_SLV,insertSubquery,lastUpdDt,tmspFld,joinCond)
  return (mrgCmd)

def getDimInsertSubQuery(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,lkpJoinList,fldLstFrmSrc,fldLstFrmTrg,ChangeTrackingType):
  surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
  if ChangeTrackingType==1:                        
    insCmd=("SELECT {9} FROM (SELECT monotonically_increasing_id()+{0} AS {7},current_timestamp() as ValidFromDate,\
    cast('9999-12-31' as timestamp) AS ValidToDate,1 as CurrentFlg, {6} FROM BRONZE.{2} AS S {8} WHERE S.{3} IS NULL OR S.{3}>='{4}')")\
    .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,fldLstFrmSrc,surrKeyName,lkpJoinList,fldLstFrmTrg)
  elif ChangeTrackingType==2:
    insCmd=("SELECT {9} FROM (SELECT monotonically_increasing_id()+{0} AS {7},current_timestamp() as ValidFromDate,1 as CurrentFlg,\
    CAST('9999-12-31' AS timestamp) AS ValidToDate, {6} FROM BRONZE.{2} AS S {8} WHERE S.{3} IS NULL OR S.{3}>='{4}')")\
     .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,fldLstFrmSrc,surrKeyName,lkpJoinList,fldLstFrmTrg)
  return (insCmd)

def getUpdInsCmdsNew(tableName_SLV,tableType,ChangeTrackingType):
  delCmd=""
  updateCmd=None
  tableName_BRZ=getBronzeTableName(tableName_SLV)          #Get BRONZE table name   
#   print (tableName_BRZ,tableName_SLV)
  sourceTargetJoinCond=getJoinConditions('SILVER',tableName_SLV,"S","T")    #Get join conditions
  lkpJoinList=getLkpJoinList("SILVER",tableName_SLV)
#   print (sourceTargetJoinCond)
  tmspFld=getKeyFieldName(tableName_SLV,"IsTimestampFlg") #Get timestamp field name
  busKeyFld=getKeyFieldName(tableName_SLV,"IsBusinessKeyFlg") #Get timestamp field name
  lastUpdDt,lastId=getWatermarkData(tableName_SLV)        #Get watermark data    
  fldListSrc=getFieldList("SILVER",tableName_SLV,"S.",False)
  if tableType=='D':
    fldListTrg=getFieldList("SILVER",tableName_SLV,"",True)
    insertSubQuery=getDimInsertSubQuery(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,lkpJoinList,fldListSrc,fldListTrg,ChangeTrackingType)
    updateCmd=getDimMergeCmdNew(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,ChangeTrackingType,insertSubQuery)
  else:
    insertSubQuery,delCmd=getFactInsertSubQuery(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldListSrc,lkpJoinList,busKeyFld,ChangeTrackingType)
    if ChangeTrackingType==1:
        updateCmd=getMergeOverwriteCmdNew(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,ChangeTrackingType,insertSubQuery)
#     insertCmd,delCmd=getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldListSrc,lkpJoinList,busKeyFld,ChangeTrackingType)
  return (updateCmd,delCmd)

def getMergeOverwriteCmdNew(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChangeTrackingType,insertSubquery):
  sqlCmd="SELECT SourceColumnName,ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsSCDFlg=False AND IsSurrogateKeyFlg=False AND \
   IsBusinessKeyFlg=False AND LookupColumnName IS NULL ORDER BY ColumnOrder".format("SILVER",tableName_SLV)
  df=spark.sql(sqlCmd)
  rddMap=df.rdd.map(lambda x:",T.{0}=S.{1}".format(x['ColumnName'],x['SourceColumnName']))
  fldUpdStr=rddMap.reduce(lambda x,y:x+y).replace(',','',1)
  mergeCmd=("MERGE INTO SILVER.{0} AS T USING ({1}) AS S ON {4} \
  WHEN MATCHED THEN UPDATE SET {5} \
  WHEN NOT MATCHED THEN INSERT * ").format(tableName_SLV,insertSubquery,lastUpdDt,tmspFld,joinCond,fldUpdStr)
  return (mergeCmd)


def ingestFactTables(tableArr,truncateFirstFlg,initialUploadFlg,printSQLCmdsFlg):
  for tableItem in tableArr:
    sqlUpdateStr,delCmd=getUpdInsCmdsNew(tableItem.TableName_SLV,'F',tableItem.ChangeTrackingType)
    if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE SILVER.{0}".format(tableItem.TableName_SLV))
#      elif initialUploadFlg==False: 
    if sqlUpdateStr is not None:
      if printSQLCmdsFlg==True:
        print(sqlUpdateStr)  
      spark.sql(sqlUpdateStr)
      updateETLLogs("SILVER",tableItem.TableName_SLV) 
    if len(delCmd)>0:
      if printSQLCmdsFlg==True:
        print(delCmd) 
      spark.sql(delCmd)
      updateETLLogs("SILVER",tableItem.TableName_SLV)
    updateETLLogs("SILVER",tableItem.TableName_SLV)
    updateWatermarkData(tableItem.TableName_SLV,'F')  
  return  

def ingestSilverTables(truncateFirstFlg,initialUploadFlg,printSQLCmdsFlg=False):
  spark.sql("CACHE TABLE ETL.TableNames")
  spark.sql("CACHE TABLE ETL.TableFields")
  populateDimDatesFromFacts()
  tbsArr=getQueryResultsAsList("Select TableName_SLV,ChangeTrackingType FROM ETL.TableNames WHERE TableName_SLV IS NOT NULL AND TableType='D' AND TableName_BRZ IS NOT NULL ")  #Dimensions
  ingestDimTables(tbsArr,truncateFirstFlg,initialUploadFlg,printSQLCmdsFlg)
  tbsArr=getQueryResultsAsList("Select TableName_SLV,ChangeTrackingType FROM ETL.TableNames WHERE TableName_SLV IS NOT NULL AND TableType='F'")
  ingestFactTables(tbsArr,truncateFirstFlg,initialUploadFlg,printSQLCmdsFlg)
  spark.sql("UNCACHE TABLE ETL.TableNames")
  spark.sql("UNCACHE TABLE ETL.TableFields")
  return

def getFactDelCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt):
    snpDtKeyAtts=getKeyFieldAttributes(tableName_SLV,"IsSnapshotKey")
    dtLstStr=delCmd=""
    snpDtCmd="SELECT DISTINCT CAST(D.DateKey AS STRING) AS SnpDtColumn FROM BRONZE.{1} T LEFT JOIN SILVER.DimDate D ON T.{0}=D.Date \
    WHERE T.{2}>=CAST('{3}' AS timestamp) ".format(snpDtKeyAtts['LookupColumnName'],tableName_BRZ,tmspFld,lastUpdDt)
    print (snpDtCmd)
    df=spark.sql(snpDtCmd)
    print (df.count())
    if df.count()>0:
        rddMap=df.rdd.map(lambda x:","+x['SnpDtColumn'])
        dtLstStr+=rddMap.reduce(lambda x,y:x+y)
    if len(dtLstStr)>1:
        delCmd="DELETE FROM SILVER.{0} WHERE {1} IN ({2}) ".format(tableName_SLV,snpDtKeyAtts['ColumnName'],dtLstStr.replace(',','',1))
    return delCmd

def getKeyFieldAttributes(tableName,keyType):
    sqlCmd="SELECT ColumnName,SourceSchemaName,SourceTableName,SourceColumnName,LookupColumnName,ColumnOrder FROM ETL.TableFields WHERE TableName='{0}' and {1}=true".format(tableName,keyType)
    df=spark.sql(sqlCmd)
    if df.count()>0:
      keyFieldAtts=df.first()
    else:
      keyFieldAtts=None
#     print  (keyFieldAtts) 
    return keyFieldAtts  
    

# COMMAND ----------

# getKeyFieldAttributes('FactSalesOrderHeader',"IsSnapshotKey")
# getFactDelCmd('FactProductInventory','ProductInventory','ModifiedDate','2020-01-01')



# COMMAND ----------

# %sql
# DESCRIBE HISTORY silver.FactProductInventory

# COMMAND ----------

# def getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,fldLstFrmSrc,lkpJoinList,busKeyFld,ChangeTrackingType):
#     delCmd=""
#     if ChangeTrackingType==1:
#       insertCmd="INSERT INTO SILVER.{1} SELECT {2} FROM BRONZE.{0} AS S LEFT JOIN SILVER.{1} AS T ON {3} {4} WHERE T.{8} IS NULL AND S.{5}>='{6}'\
#        ".format(tableName_BRZ,tableName_SLV,fldLstFrmSrc,srcTrgJoinCond,lkpJoinList,tmspFld,lastUpdDt,srcTrgJoinCond,busKeyFld)
#     elif ChangeTrackingType==2:
#       delCmd=getFactDelCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt)
#       insertCmd="INSERT INTO SILVER.{1} SELECT {2} FROM BRONZE.{0} AS S {4} WHERE S.{5}>='{6}'\
#        ".format(tableName_BRZ,tableName_SLV,fldLstFrmSrc,srcTrgJoinCond,lkpJoinList,tmspFld,lastUpdDt,srcTrgJoinCond,busKeyFld)
#     print (insertCmd)
# #   print("Ingestiion for {0} is complete".format(tableItem.TableName_BRZ))
#     return (insertCmd,delCmd)   #Add update statement

# def getDimMergeCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChangeTrackingType):
#   if ChangeTrackingType==1:  
#     mrgCmd=getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChangeTrackingType)
#   elif ChangeTrackingType==2:
#     mrgCmd="MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
#     WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp(),CurrentFlg=0".format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond)
#   return (mrgCmd)
# def getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChangeTrackingType):
#   sqlCmd="SELECT SourceColumnName,ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsSCDFlg=False AND IsSurrogateKeyFlg=False AND \
#    IsBusinessKeyFlg=False AND LookupColumnName IS NULL ORDER BY ColumnOrder".format("SILVER",tableName_SLV)
#   df=spark.sql(sqlCmd)
#   rddMap=df.rdd.map(lambda x:",T.{0}=S.{1}".format(x['ColumnName'],x['SourceColumnName']))
#   fldUpdStr=rddMap.reduce(lambda x,y:x+y).replace(',','',1)
#   mergeCmd=("MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
#   WHEN MATCHED THEN UPDATE SET {5}").format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond,fldUpdStr)
#   return (mergeCmd)

# def getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,lkpJoinList,fldLstFrmSrc,fldLstFrmTrg,ChangeTrackingType):
#   surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
# #   fnSKLst=getFnSkList("SILVER",tableName_SLV)
#   if ChangeTrackingType==1:                        
#     insCmd=("INSERT INTO SILVER.{1} SELECT {9} FROM (SELECT monotonically_increasing_id()+{0} AS {7},current_timestamp() as ValidFromDate,\
#     cast('9999-12-31' as timestamp) AS ValidToDate,1 as CurrentFlg, {6} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} {8} WHERE T.{7} IS NULL AND (S.{3} IS NULL OR S.{3}>='{4}'))")\
#     .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,fldLstFrmSrc,surrKeyName,lkpJoinList,fldLstFrmTrg)
#   elif ChangeTrackingType==2:
#     insCmd=("INSERT INTO SILVER.{1} SELECT {9} FROM (SELECT monotonically_increasing_id()+{0} AS {7},current_timestamp() as ValidFromDate,1 as CurrentFlg,\
#     CAST('9999-12-31' AS timestamp) AS ValidToDate, {6} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} {8} WHERE (S.{3} IS NULL OR S.{3}>='{4}'))")\
#      .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,srcTrgJoinCond,fldLstFrmSrc,surrKeyName,lkpJoinList,fldLstFrmTrg)
#   return (insCmd)

# def getUpdInsCmds(tableName_SLV,tableType,ChangeTrackingType):
#   delCmd=""
#   updateCmd=None
#   tableName_BRZ=getBronzeTableName(tableName_SLV)          #Get BRONZE table name   
# #   print (tableName_BRZ,tableName_SLV)
#   sourceTargetJoinCond=getJoinConditions('SILVER',tableName_SLV,"S","T")    #Get join conditions
#   lkpJoinList=getLkpJoinList("SILVER",tableName_SLV)
# #   print (sourceTargetJoinCond)
#   tmspFld=getKeyFieldName(tableName_SLV,"IsTimestampFlg") #Get timestamp field name
#   busKeyFld=getKeyFieldName(tableName_SLV,"IsBusinessKeyFlg") #Get timestamp field name
#   lastUpdDt,lastId=getWatermarkData(tableName_SLV)        #Get watermark data    
#   fldListSrc=getFieldList("SILVER",tableName_SLV,"S.",False)
#   if tableType=='D':
#     fldListTrg=getFieldList("SILVER",tableName_SLV,"",True)
# #     fldLstStr=getFieldList("SILVER",tableName_SLV,True,"")
#     updateCmd=getDimMergeCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,ChangeTrackingType)
#     insertCmd=getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,lkpJoinList,fldListSrc,fldListTrg,ChangeTrackingType)
#   else:
#     if ChangeTrackingType==1:
#       updateCmd=getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,ChangeTrackingType)
#     insertCmd,delCmd=getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldListSrc,lkpJoinList,busKeyFld,ChangeTrackingType)
#   return (insertCmd,updateCmd,delCmd)









# def getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond):
#   sqlCmd="SELECT SourceColumnName,ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsSCDFlg=False AND IsSurrogateKeyFlg=False AND \
#    IsBusinessKeyFlg=False AND IsForeignKeyFlg=False ORDER BY ColumnOrder".format("SILVER",tableName_SLV)
#   df=spark.sql(sqlCmd)
#   rddMap=df.rdd.map(lambda x:",T.{0}=S.{1}".format(x['ColumnName'],x['SourceColumnName']))
#   fldUpdStr=rddMap.reduce(lambda x,y:x+y).replace(',','',1)
#   mergeCmd=("MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
#   WHEN MATCHED THEN UPDATE SET {5}").format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond,fldUpdStr)
#   return (mergeCmd)
# def getFieldList(SchemaName,TableName,includeForeignKeyFlg=False,tableAlias="",replaceForeignKeys=False):
#   if includeForeignKeyFlg ==False:
#     fkCondition="IsForeignKeyFlg=false"
#   else:
#     fkCondition="1=1"
#   sqlKeys="SELECT ColumnName,SourceColumnName,SourceTableName,LookupColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND {2} ORDER BY ColumnOrder".format(SchemaName,TableName,fkCondition)
#   rdd=spark.sql(sqlKeys).rdd.map(lambda x:','+tableAlias+x['ColumnName'])
#   colLstStr=rdd.reduce(lambda x,y:x+y).replace(",","",1)
#   joinAliasNum=1
#   if replaceForeignKeys==True:
#     sqlKeys="SELECT ColumnName,ForeignKeyTableName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsForeignKeyFlg=True ORDER BY ColumnOrder".format(SchemaName,TableName)
#     keyRows=spark.sql(sqlKeys).collect()
#     for keyRow in keyRows:
#       primaryColumn,fkTable=keyRow
#       surrKeyName=getKeyFieldName(fkTable,"IsSurrogateKeyFlg")
#       surrKeyExp="D{0}.{1}".format(joinAliasNum,surrKeyName)
#       refKeyExp="{0}{1}".format(tableAlias,primaryColumn)
# #       print (refKeyExp,surrKeyExp)
#       colLstStr=colLstStr.replace(refKeyExp,surrKeyExp)
#       joinAliasNum+=1
# #   print(colLstStr)   
#   return colLstStr
# def getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,ChgHstTrackingType):
#   surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
#   fnSKLst=getFnSkList("SILVER",tableName_SLV)
#   if   ChgHstTrackingType==1:                        
#     insCmd=("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT monotonically_increasing_id()+{0} AS {7},S.*,current_timestamp() as ValidFromDate,\
#     cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} WHERE T.{7} IS NULL AND (S.{3} IS NULL OR S.{3}>='{4}'))")\
#     .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst)
#   elif ChgHstTrackingType==2:
#     insCmd=("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT monotonically_increasing_id()+{0} AS {7},S.*,current_timestamp() as ValidFromDate,\
#     cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} WHERE (S.{3} IS NULL OR S.{3}>='{4}'))")\
#      .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst)
#   return (insCmd)

# def getMatchSQLCmd(lastId,dimTableName,brzTableName,tmspFld,lastUpdDt,joinCond,matchFlg):
#   if matchFlg==True:
#     matchCond="not null"
#   else: 
#     matchCond= "null" 
#   return (("SELECT (ROW_NUMBER() OVER (ORDER BY S.{3})+{0}) AS DimSurrKey,S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate\
#  FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} where T.{3} is {6} and S.{3}>='{4}'").format(lastId,dimTableName,brzTableName,tmspFld,lastUpdDt,joinCond,matchCond))




# fkStructType=StructType([StructField("primFieldName",StringType(),True),
#                      StructField("fnKeyTableName",StringType(),True),
#                      StructField("fnSurrKey",StringType(),True),
#                      StructField("fnBussKeyName",StringType(),True)])


# def getFnSkList(schemaName,tableName):
#     fnKeysArr=getFnSurrKeyMetadata("SILVER",tableName)
#     fnSurrKeyLst=""
#     if len(fnKeysArr)>0:
#       for fnSet in fnKeysArr:
#         fnSurrKeyLst+=(','+fnSet[2])
#     return fnSurrKeyLst

# def getFnSurrKeyMetadata(schemaName,tableName):
#   sqlKeys="SELECT ColumnName,SourceColumnName,ForeignKeyTableName FROM ETL.TableFields WHERE SchemaName='{1}' AND TableName='{0}' and IsForeignKeyFlg=true".format(tableName,schemaName)
# #   fkKeyArr =getKeyFieldsArr(schemaName,tableName,"IsForeignKeyFlg")
#   fkKeyArr =spark.sql(sqlKeys).collect()
#   fkArr=[]
#   fnSurrKeyLst=""
#   for fk in fkKeyArr:
#     primFieldName,primFieldNameRef,fnKeyTableName=fk
#     fnSurrKey=getKeyFieldName(fnKeyTableName,"IsSurrogateKeyFlg") 
#     fnBussKeyName=getKeyFieldName(fnKeyTableName,"IsBusinessKeyFlg")
#     fnTimestampName=getKeyFieldName(fnKeyTableName,"IsTimestampFlg")
# #     print(primFieldName,fnKeyTableName,fnSurrKey,fnBussKeyName)
#     fkArr.append([primFieldName,fnKeyTableName,fnSurrKey,fnBussKeyName,primFieldNameRef])
#   return fkArr

# def getDimResults_Old(DimTableName):
#   joinCond=getJoinConditions(DimTableName)               #Get join conditions
#   tmspFld=getTimestampField(DimTableName)                #Get timestamp field name
#   lastUpdDt,lastId=getWatermarkData(DimTableName)        #Get watermark data    

#   sqlStrMatch=("SELECT monotonically_increasing_id() AS CustomerKey,S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is not null and S.{4}>='{2}'").format(DimTableName,keyFieldName,lastUpdDt,lastId,tmspFld)
#   print (sqlStrMatch)
# #   dfM=spark.sql(sqlStrMatch)
  
#   updStrMatch=("MERGE INTO {0} AS T USING {0}_RAW AS S ON "+joinCond+" AND S.{3}>='{2}'  WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp() ").format(DimTableName,keyFieldName,lastUpdDt,tmspFld)
#   print (updStrMatch)
# #   dfM=spark.sql(updStrMatch)
  
# #   sqlStrNoMatch=("SELECT (ROW_NUMBER() OVER (ORDER BY S.{1})+{2}) AS CustomerKey, S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate  FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is null").format(TableName,keyFieldName,lastId)
#   sqlStrNoMatch=("SELECT monotonically_increasing_id() AS CustomerKey, S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate  FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is null").format(DimTableName,keyFieldName,lastId)  
# #   dfNM=spark.sql(sqlStrNoMatch)
  
#   return (sqlStrNoMatch,sqlStrMatch,updStrMatch)
  
  
#   def getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr):
#   surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
#   fnSKLst=getFnSkList("SILVER",tableName_SLV)
#   return (("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT (ROW_NUMBER() OVER (ORDER BY S.{3})+{0}) AS {7},S.*,current_timestamp() as ValidFromDate,\
#   cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} where T.{3} IS NULL AND (S.{3} IS NULL OR S.{3}>='{4}'))")
#    .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst))


# COMMAND ----------

# MAGIC %sql
# MAGIC -- select * from etl.logs where tableName='DimCustomer' order by updatedatetime desc
# MAGIC --order by schemaName,tableName

# COMMAND ----------

# df=spark.table("BRONZE.SalesLTProductDescription")
# # df.rdd.partitions.size
# # df.show()
# df.rdd.getNumPartitions()

# COMMAND ----------

