# Databricks notebook source
# blobaccname=dbutils.secrets.get(scope="AiKvScope2", key="blobAccName")
# blobkey=dbutils.secrets.get(scope="AiKvScope2", key="blobkey")
# dbutils.fs.mount(source = "wasbs://mda@{0}.blob.core.windows.net".format(blobaccname),mount_point = "/mnt/mda/",
#     extra_configs = {"fs.azure.account.key.{0}.blob.core.windows.net".format(blobaccname):"{0}".format(blobkey)})

# COMMAND ----------

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.types import StructType

# COMMAND ----------

# MAGIC %md ####Create metadata tables

# COMMAND ----------

def createMetaTables():
  spark.sql("CREATE DATABASE IF NOT EXISTS BRONZE")
  spark.sql("CREATE DATABASE IF NOT EXISTS SILVER")
  spark.sql("CREATE DATABASE IF NOT EXISTS GOLD")
  spark.sql("CREATE DATABASE IF NOT EXISTS ETL")
  spark.sql("drop table if exists ETL.TableNames")
  sqlStr="create  table ETL.TableNames (\
  TableName_GLD string,\
  TableName_SLV string,\
  TableName_BRZ string,\
  SourcePath string,\
  TableType string,\
  ChgHstTrackingType int,\
  WatermarkDate timestamp,\
  LastId int)\
  USING DELTA"
  spark.sql(sqlStr)
  #TableFields
  spark.sql("drop table if exists ETL.TableFields")
  sqlStr="create  table ETL.TableFields (\
  SchemaName string,\
  TableName string,\
  ColumnName string,\
  SourceColumnName string,\
  ColumnOrder int,\
  ColumnType string,\
  IsSurrogateKeyFlg boolean,\
  IsBusinessKeyFlg boolean,\
  IsTimestampFlg boolean,\
  IsForeignKeyFlg boolean,\
  IsPartitionKeyFlg boolean,\
  IsSCDFlg boolean,\
  ForeignKeyTableName string)\
  USING DELTA"
  spark.sql(sqlStr)
  #Logs
  spark.sql("drop table if exists ETL.Logs")
  sqlStr="create table ETL.Logs (\
  SchemaName string,\
  TableName string,\
  Operation string,\
  InsertedRows int,\
  UpdatedRows int,\
  UpdateDateTime timestamp)\
  USING DELTA"
  spark.sql(sqlStr)
  print ("Metadata tables have been created")
  return

# COMMAND ----------

def InitializeMetadata():
  createMetaTables()
  populateTableNames()
  populateTableFields()
  return

# COMMAND ----------

# MAGIC %md ####Populate metadata tables

# COMMAND ----------

def populateTableNames():
  spark.sql("truncate table ETL.TableNames")
  spark.sql("drop table if exists ETL.TableNames_STG")
  sqlStr="create  table ETL.TableNames_STG (\
  TableName_GLD string,\
  TableName_SLV string,\
  TableName_BRZ string,\
  SourcePath string,\
  TableType string,\
  ChgHstTrackingType int,\
  WatermarkDate string,\
  LastId int\
  )\
  USING CSV\
  OPTIONS (header 'true')\
  LOCATION '/mnt/mda/source/Metadata/Metadata_Tables.csv'"
  spark.sql(sqlStr)      
  spark.sql("INSERT INTO ETL.TableNames SELECT TableName_GLD,TableName_SLV,TableName_BRZ,SourcePath,TableType,ChgHstTrackingType,\
    CAST(WatermarkDate AS timestamp),LastId FROM ETL.TableNames_STG  WHERE TableName_BRZ is not null")
  return

def populateTableFields():
  spark.sql("truncate table ETL.TableFields")
  spark.sql("drop table if exists ETL.TableFields_STG")
  sqlStr="create  table ETL.TableFields_STG (\
  SchemaName string,\
  TableName string,\
  ColumnName string,\
  SourceColumnName string,\
  ColumnOrder int,\
  ColumnType string,\
  IsSurrogateKeyFlg int,\
  IsBusinessKeyFlg int,\
  IsTimestampFlg int,\
  IsForeignKeyFlg int,\
  IsPartitionKeyFlg int,\
  IsSCDFlg int,\
  ForeignKeyTableName string\
  )\
  USING CSV\
  OPTIONS (header 'true')\
  LOCATION '/mnt/mda/source/Metadata/Metadata_Columns.csv'"
  spark.sql(sqlStr)           
  spark.sql("insert into ETL.TableFields  select SchemaName,TableName,ColumnName,SourceColumnName,ColumnOrder,ColumnType,IsSurrogateKeyFlg,IsBusinessKeyFlg,IsTimestampFlg,\
    IsForeignKeyFlg,IsPartitionKeyFlg,IsSCDFlg,ForeignKeyTableName from ETL.TableFields_STG WHERE TableName is not null ORDER BY TableName,ColumnOrder")
  return

# COMMAND ----------

# MAGIC %md ####Create DWH tables

# COMMAND ----------

def generateCreateStr(tableName,tableSchema):
   partitionKey=getKeyFieldName(tableName,"IsPartitionKeyFlg")
   dropCmdStr="DROP TABLE IF EXISTS {1}.{0}".format(tableName,tableSchema) 
   createCmdStr="CREATE TABLE {1}.{0} ( ".format(tableName,tableSchema)
   dfCols=spark.sql("SELECT ColumnName,ColumnType FROM ETL.TableFields WHERE TableName='{0}' ORDER BY ColumnOrder".format(tableName))
   rdd=dfCols.rdd.map(lambda x:','+x['ColumnName']+'  '+x['ColumnType'])
   createCmdStr+=rdd.reduce(lambda x,y:x+y).replace(",","",1)
   createCmdStr+=") USING DELTA "
   if partitionKey is not None:
     createCmdStr+=" PARTITIONED BY ({0})".format(partitionKey)
   return (dropCmdStr,createCmdStr)

def createDWHTables(SchemaName):
  if SchemaName=="BRONZE":
    TableColNm="TableName_BRZ"
  else:
    TableColNm="TableName_SLV"
  dfTabs=spark.sql("SELECT {0} FROM ETL.TableNames".format(TableColNm))
  tabLst=dfTabs.collect()
  for row in tabLst:
    dropBrzStr,createBrzStr=generateCreateStr(row[TableColNm],SchemaName)
#     print (createBrzStr)
    spark.sql(dropBrzStr)
    spark.sql(createBrzStr)
    print("Table {} has been created".format(row[TableColNm]))
  return        

# COMMAND ----------

# MAGIC %md ####Utility functions

# COMMAND ----------

def getWatermarkData(TableName):
  sqlStrWtrm=("SELECT WatermarkDate,LastId FROM ETL.TableNames where TableName_SLV='{0}'").format(TableName)
  dfWM=spark.sql(sqlStrWtrm)
  return (dfWM.first())  

def updateWatermarkData(TableName,TableType):
  if TableType=='D':
    surrKeyName=getKeyFieldName(TableName,"IsSurrogateKeyFlg") 
    sqlMaxVal="SELECT MAX({1}) AS MaxID FROM SILVER.{0} ".format(TableName,surrKeyName)
    maxID=spark.sql(sqlMaxVal).first()['MaxID']
#   print(maxID)
    if maxID is not None:
      sqlStrWtrm=("UPDATE ETL.TableNames SET WatermarkDate =current_timestamp(),LastId={0} WHERE TableName_SLV='{1}'").format(maxID,TableName)
      dfWM=spark.sql(sqlStrWtrm)
    else:
      print ("Table {0} has no rows".format(TableName))  
#     print (sqlStrWtrm)
  else:
      sqlStrWtrm=("UPDATE ETL.TableNames SET WatermarkDate =current_timestamp() WHERE TableName_SLV='{0}'").format(TableName)
      dfWM=spark.sql(sqlStrWtrm)
  return 

def getKeyFieldName(tableName,keyType):
    sqlKeys="SELECT ColumnName FROM ETL.TableFields WHERE TableName='{0}' and {1}=true".format(tableName,keyType)
    df=spark.sql(sqlKeys)
    if df.count()>0:
      keyFieldName=df.first()['ColumnName']
    else:
      keyFieldName=None
    return keyFieldName

def getFieldList(SchemaName,TableName,includeForeignKeyFlg=False,tableAlias="",replaceForeignKeys=False):
  if includeForeignKeyFlg ==False:
    fkCondition="IsForeignKeyFlg=false"
  else:
    fkCondition="1=1"
  sqlKeys="SELECT ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND {2} ORDER BY ColumnOrder".format(SchemaName,TableName,fkCondition)
  rdd=spark.sql(sqlKeys).rdd.map(lambda x:','+tableAlias+x['ColumnName'])
  colLstStr=rdd.reduce(lambda x,y:x+y).replace(",","",1)
  joinAliasNum=1
  if replaceForeignKeys==True:
    sqlKeys="SELECT ColumnName,ForeignKeyTableName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsForeignKeyFlg=True ORDER BY ColumnOrder".format(SchemaName,TableName)
    keyRows=spark.sql(sqlKeys).collect()
    for keyRow in keyRows:
      primaryColumn,fkTable=keyRow
      surrKeyName=getKeyFieldName(fkTable,"IsSurrogateKeyFlg")
      surrKeyExp="D{0}.{1}".format(joinAliasNum,surrKeyName)
      refKeyExp="{0}{1}".format(tableAlias,primaryColumn)
#       print (refKeyExp,surrKeyExp)
      colLstStr=colLstStr.replace(refKeyExp,surrKeyExp)
      joinAliasNum+=1
#   print(colLstStr)   
  return colLstStr
 
def getBronzeTableName(tableName):
  sqlStrWtrm=("SELECT TableName_BRZ FROM ETL.TableNames where TableName_SLV='{0}'").format(tableName)
  dfWM=spark.sql(sqlStrWtrm)
  return (dfWM.first()['TableName_BRZ']) 

def getRowCounts():
  sqlCmd="SELECT TableName_BRZ,TableName_SLV FROM ETL.TableNames "
  dfRows=spark.sql(sqlCmd).collect()
  for row in dfRows:
    rowCnt_BRZ=spark.table("BRONZE.{0}".format(row['TableName_BRZ'])).count()
    rowCnt_SLV=spark.table("SILVER.{0}".format(row['TableName_SLV'])).count()
    print ("Row counts- BRONZE.{0}:{1}, SILVER.{2}:{3}".format(row['TableName_BRZ'],rowCnt_BRZ,row['TableName_SLV'],rowCnt_SLV))                       
  return

def resetWatermarkData():
  spark.sql("update ETL.TableNames set LastId=0,WatermarkDate='1900-01-01'")
  return

def cleanDWHTables(schemaName):
  sqlCmd="SELECT TableName_BRZ,TableName_SLV FROM ETL.TableNames "
  dfRows=spark.sql(sqlCmd).collect()
  for row in dfRows:
    if schemaName=="SILVER":
      spark.sql("TRUNCATE TABLE SILVER.{0}".format(row['TableName_SLV']))
      resetWatermarkData()
      print ("Table SILVER.{} has been cleaned".format(row['TableName_SLV']))
    else:   
      spark.sql("TRUNCATE TABLE BRONZE.{0}".format(row['TableName_BRZ']))  
      print ("Table BRONZE.{} has been cleaned".format(row['TableName_BRZ']))
  return

def updateETLLogs(schemaName,tableName):
  sqlCmd="DESCRIBE HISTORY {0}.{1}".format(schemaName,tableName)
#   print (sqlCmd)
  df=spark.sql(sqlCmd)
  row=df.orderBy(df.version, ascending=False).selectExpr("operation","operationParameters.mode As UpdateMode","operationMetrics.numOutputRows As TotalRows",\
     "operationMetrics.numUpdatedRows As UpdatedRows","operationMetrics.numTargetRowsUpdated As MergedRows","timestamp").first()
  insCnt=0
  updCnt=0
  if row["operation"]=="UPDATE":
    updCnt=row["UpdatedRows"]
  elif row["operation"]=="MERGE":
    updCnt=row["MergedRows"]
  elif row["operation"]=="WRITE":
    insCnt=row["TotalRows"]
  insertCmd="INSERT INTO ETL.Logs VALUES ('{0}','{1}','{2}',{3},{4},CAST('{5}' as timestamp))".format(schemaName,tableName,row["operation"],insCnt,updCnt,row["timestamp"])
  spark.sql(insertCmd)
  return

# COMMAND ----------

# MAGIC %md ####BRONZE level ingestions

# COMMAND ----------

def ingestBronzeTables(truncateFirstFlg):
  df=spark.sql("Select TableName_BRZ,SourcePath from ETL.TableNames")
  tbsArr=df.collect()
  for tableItem in tbsArr:
      sourcePath="{0}{1}.csv".format(tableItem.SourcePath,tableItem.TableName_BRZ)
      df=spark.read.csv(sourcePath,header=True)
      df.createOrReplaceTempView("TempSource")
      fldLstStr=getFieldList("BRONZE",tableItem.TableName_BRZ,False,"",False)
      sqlStr="INSERT INTO BRONZE.{0} SELECT {1} FROM TempSource".format(tableItem.TableName_BRZ,fldLstStr)
      print("Ingestion for table {0} is complete".format(tableItem.TableName_BRZ))
#       print (sqlStr)
      if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE BRONZE.{0}".format(tableItem.TableName_BRZ))
      spark.sql(sqlStr)
      updateETLLogs("BRONZE",tableItem.TableName_BRZ)
  print(df.count())
  return  


# COMMAND ----------

# MAGIC %md ####SILVER level ingestions

# COMMAND ----------

# def getMatchSQLCmd(lastId,dimTableName,brzTableName,tmspFld,lastUpdDt,joinCond,matchFlg):
#   if matchFlg==True:
#     matchCond="not null"
#   else: 
#     matchCond= "null" 
#   return (("SELECT (ROW_NUMBER() OVER (ORDER BY S.{3})+{0}) AS DimSurrKey,S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate\
#  FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} where T.{3} is {6} and S.{3}>='{4}'").format(lastId,dimTableName,brzTableName,tmspFld,lastUpdDt,joinCond,matchCond))




# fkStructType=StructType([StructField("primFieldName",StringType(),True),
#                      StructField("fnKeyTableName",StringType(),True),
#                      StructField("fnSurrKey",StringType(),True),
#                      StructField("fnBussKeyName",StringType(),True)])




def getJoinConditions(schemaName,tableName):
  sqlCmd="SELECT ColumnName,SourceColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' and IsBusinessKeyFlg=true".format(schemaName,tableName)
  df=spark.sql(sqlCmd)
  if df.count()>0:
      rddMap=df.rdd.map(lambda x:" AND S.{0}=T.{1} ".format(x['SourceColumnName'],x['ColumnName']))
      joinCond=rddMap.reduce(lambda x,y:x+y).replace("AND","",1)  
  return joinCond

def getKeyFieldsList(schemaName,tableName,keyType):
    sqlKeys="SELECT ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' and {2}=true".format(schemaName,tableName,keyType)
    df=spark.sql(sqlKeys)
    if df.count()>0:
      rddMap=df.rdd.map(lambda x:','+x['ColumnName'])
      colLstStr=rddMap.reduce(lambda x,y:x+y)
    else:
      colLstStr=""
    return colLstStr

def getFnSkList(schemaName,tableName):
    fnKeysArr=getFnSurrKeyMetadata("SILVER",tableName)
    fnSurrKeyLst=""
    if len(fnKeysArr)>0:
      for fnSet in fnKeysArr:
        fnSurrKeyLst+=(','+fnSet[2])
    return fnSurrKeyLst
  
def getFnSurrKeyMetadata(schemaName,tableName):
  sqlKeys="SELECT ColumnName,SourceColumnName,ForeignKeyTableName FROM ETL.TableFields WHERE SchemaName='{1}' AND TableName='{0}' and IsForeignKeyFlg=true".format(tableName,schemaName)
#   fkKeyArr =getKeyFieldsArr(schemaName,tableName,"IsForeignKeyFlg")
  fkKeyArr =spark.sql(sqlKeys).collect()
  fkArr=[]
  fnSurrKeyLst=""
  for fk in fkKeyArr:
    primFieldName,primFieldNameRef,fnKeyTableName=fk
    fnSurrKey=getKeyFieldName(fnKeyTableName,"IsSurrogateKeyFlg") 
    fnBussKeyName=getKeyFieldName(fnKeyTableName,"IsBusinessKeyFlg")
    fnTimestampName=getKeyFieldName(fnKeyTableName,"IsTimestampFlg")
#     print(primFieldName,fnKeyTableName,fnSurrKey,fnBussKeyName)
    fkArr.append([primFieldName,fnKeyTableName,fnSurrKey,fnBussKeyName,primFieldNameRef])
  return fkArr

def ingestDimTables(tableArr,truncateFirstFlg,initialUploadFlg):
  for tableItem in tableArr:
     sqlInsertStr,sqlUpdateStr=getUpdInsCmds(tableItem.TableName_SLV,'D',tableItem.ChgHstTrackingType)
     if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE SILVER.{0}".format(tableItem.TableName_SLV))
     elif initialUploadFlg==False:  
        print(sqlUpdateStr)
        spark.sql(sqlUpdateStr)
        updateETLLogs("SILVER",tableItem.TableName_SLV) 
     print(sqlInsertStr) 
#     print("Ingesting into table {0} is complete".format(tableItem.TableName_SLV))
     spark.sql(sqlInsertStr)
     updateETLLogs("SILVER",tableItem.TableName_SLV) 
     updateWatermarkData(tableItem.TableName_SLV,'D') 
#      print(sqlUpdateStr)
  return  

def getDimMergeCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,ChgHstTrackingType):
  if   ChgHstTrackingType==1:  
    mrgCmd=getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond)
  elif ChgHstTrackingType==2:
    mrgCmd=("MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
    WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp()").format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond)
  return (mrgCmd)

def getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,ChgHstTrackingType):
  surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
  fnSKLst=getFnSkList("SILVER",tableName_SLV)
  if   ChgHstTrackingType==1:                        
    insCmd=("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT monotonically_increasing_id()+{0} AS {7},S.*,current_timestamp() as ValidFromDate,\
    cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} WHERE T.{7} IS NULL AND (S.{3} IS NULL OR S.{3}>='{4}'))")\
    .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst)
  elif ChgHstTrackingType==2:
    insCmd=("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT monotonically_increasing_id()+{0} AS {7},S.*,current_timestamp() as ValidFromDate,\
    cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} WHERE (S.{3} IS NULL OR S.{3}>='{4}'))")\
     .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst)
  return (insCmd)
 
def getUpdInsCmds(tableName_SLV,tableType,ChgHstTrackingType):
  tableName_BRZ=getBronzeTableName(tableName_SLV)          #Get BRONZE table name   
#   print (tableName_BRZ,tableName_SLV)
  sourceTargetJoinCond=getJoinConditions('SILVER',tableName_SLV)    #Get join conditions
#   print (sourceTargetJoinCond)
  tmspFld=getKeyFieldName(tableName_SLV,"IsTimestampFlg") #Get timestamp field name
  lastUpdDt,lastId=getWatermarkData(tableName_SLV)        #Get watermark data    
  if tableType=='D':
    fldLstStr=getFieldList("SILVER",tableName_SLV,True,"")
    updateCmd=getDimMergeCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,ChgHstTrackingType)
    insertCmd=getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldLstStr,ChgHstTrackingType)
  else:
    fldLstStr =getFieldList("SILVER",tableName_SLV,True,"S.",True)
    updateCmd=getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond)
    insertCmd=getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldLstStr)
  return (insertCmd,updateCmd)

def getMergeOverwriteCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond):
  sqlCmd="SELECT SourceColumnName,ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsSCDFlg=False AND IsSurrogateKeyFlg=False AND IsBusinessKeyFlg=False AND IsTimestampFlg=False\
    AND IsForeignKeyFlg=False ORDER BY ColumnOrder".format("SILVER",tableName_SLV)
  df=spark.sql(sqlCmd)
  rddMap=df.rdd.map(lambda x:",T.{0}=S.{1}".format(x['ColumnName'],x['SourceColumnName']))
  fldUpdStr=rddMap.reduce(lambda x,y:x+y).replace(',','',1)
  fldUpdStr+=",{0}=current_timestamp() ".format(tmspFld)
  mergeCmd=("MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
  WHEN MATCHED THEN UPDATE SET {5}").format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond,fldUpdStr)
  return (mergeCmd)

def getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldListStr):
    fnSurrKeyLst=""
    joinStr=""
    aliasNum=1
    fnKeysArr=getFnSurrKeyMetadata("SILVER",tableName_SLV)
    for fnSet in fnKeysArr:
      joinStr+= " LEFT JOIN SILVER.{1} AS D{4} ON S.{3}=D{4}.{2} AND D{4}.ValidToDate=CAST('9999-12-31' as timestamp)".format(fnSet[0],fnSet[1],fnSet[3],fnSet[4],aliasNum)
      aliasNum+=1
    insertCmd="INSERT INTO SILVER.{1} SELECT {2} FROM BRONZE.{0} AS S LEFT JOIN SILVER.{1} AS T ON {3}  {6} WHERE T.{7} IS NULL AND S.{4}>='{5}'\
     ".format(tableName_BRZ,tableName_SLV,fldListStr,sourceTargetJoinCond,tmspFld,lastUpdDt,joinStr,fnSet[3])
    print (insertCmd)
#   print("Ingestiion for {0} is complete".format(tableItem.TableName_BRZ))
    return (insertCmd)   #Add update statement

def ingestFactTables(tableArr,truncateFirstFlg,initialUploadFlg):
  for tableItem in tableArr:
     sqlInsertStr,sqlUpdateStr=getUpdInsCmds(tableItem.TableName_SLV,'F',tableItem.ChgHstTrackingType)
     if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE SILVER.{0}".format(tableItem.TableName_SLV))
     elif initialUploadFlg==False:   
        print(sqlUpdateStr)  
        spark.sql(sqlUpdateStr)
        updateETLLogs("SILVER",tableItem.TableName_SLV) 
     print(sqlInsertStr) 
     spark.sql(sqlInsertStr) 
     updateETLLogs("SILVER",tableItem.TableName_SLV)
     updateWatermarkData(tableItem.TableName_SLV,'F')  
#      print(sqlUpdateStr)
  return  

def ingestSilverTables(truncateFirstFlg,initialUploadFlg):
  df=spark.sql("Select TableName_SLV,ChgHstTrackingType FROM ETL.TableNames WHERE TableType='D'")
  tbsArr=df.collect()
  ingestDimTables(tbsArr,truncateFirstFlg,initialUploadFlg)
  df=spark.sql("Select TableName_SLV,ChgHstTrackingType FROM ETL.TableNames WHERE TableType='F'")
  tbsArr=df.collect()
  ingestFactTables(tbsArr,truncateFirstFlg,initialUploadFlg)
  return


# COMMAND ----------

# def getDimResults_Old(DimTableName):
#   joinCond=getJoinConditions(DimTableName)               #Get join conditions
#   tmspFld=getTimestampField(DimTableName)                #Get timestamp field name
#   lastUpdDt,lastId=getWatermarkData(DimTableName)        #Get watermark data    

#   sqlStrMatch=("SELECT monotonically_increasing_id() AS CustomerKey,S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is not null and S.{4}>='{2}'").format(DimTableName,keyFieldName,lastUpdDt,lastId,tmspFld)
#   print (sqlStrMatch)
# #   dfM=spark.sql(sqlStrMatch)
  
#   updStrMatch=("MERGE INTO {0} AS T USING {0}_RAW AS S ON "+joinCond+" AND S.{3}>='{2}'  WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp() ").format(DimTableName,keyFieldName,lastUpdDt,tmspFld)
#   print (updStrMatch)
# #   dfM=spark.sql(updStrMatch)
  
# #   sqlStrNoMatch=("SELECT (ROW_NUMBER() OVER (ORDER BY S.{1})+{2}) AS CustomerKey, S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate  FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is null").format(TableName,keyFieldName,lastId)
#   sqlStrNoMatch=("SELECT monotonically_increasing_id() AS CustomerKey, S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate  FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is null").format(DimTableName,keyFieldName,lastId)  
# #   dfNM=spark.sql(sqlStrNoMatch)
  
#   return (sqlStrNoMatch,sqlStrMatch,updStrMatch)
  
  
#   def getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr):
#   surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
#   fnSKLst=getFnSkList("SILVER",tableName_SLV)
#   return (("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT (ROW_NUMBER() OVER (ORDER BY S.{3})+{0}) AS {7},S.*,current_timestamp() as ValidFromDate,\
#   cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} where T.{3} IS NULL AND (S.{3} IS NULL OR S.{3}>='{4}'))")
#    .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst))


# COMMAND ----------

# MAGIC %sql
# MAGIC -- select * from etl.logs where tableName='DimCustomer' order by updatedatetime desc
# MAGIC --order by schemaName,tableName

# COMMAND ----------

# df=spark.table("BRONZE.SalesLTProductDescription")
# # df.rdd.partitions.size
# # df.show()
# df.rdd.getNumPartitions()

# COMMAND ----------

