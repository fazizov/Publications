# Databricks notebook source
# blobaccname=dbutils.secrets.get(scope="AiKvScope2", key="blobAccName")
# blobkey=dbutils.secrets.get(scope="AiKvScope2", key="blobkey")
# dbutils.fs.mount(source = "wasbs://mda@{0}.blob.core.windows.net".format(blobaccname),mount_point = "/mnt/mda/",
#     extra_configs = {"fs.azure.account.key.{0}.blob.core.windows.net".format(blobaccname):"{0}".format(blobkey)})

# COMMAND ----------

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.types import StructType

# COMMAND ----------

# MAGIC %md ####Create metadata tables

# COMMAND ----------

def createMetaTables():
  spark.sql("CREATE DATABASE IF NOT EXISTS BRONZE")
  spark.sql("CREATE DATABASE IF NOT EXISTS SILVER")
  spark.sql("CREATE DATABASE IF NOT EXISTS GOLD")
  spark.sql("CREATE DATABASE IF NOT EXISTS ETL")
  spark.sql("drop table if exists ETL.TableNames")
  sqlStr="create  table ETL.TableNames (\
  TableName_GLD string,\
  TableName_SLV string,\
  TableName_BRZ string,\
  SourcePath string,\
  TableType string,\
  ChangeType int,\
  WatermarkDate timestamp,\
  LastId int)\
  USING DELTA"
  spark.sql(sqlStr)
  spark.sql("drop table if exists ETL.TableFields")
  sqlStr="create  table ETL.TableFields (\
  SchemaName string,\
  TableName string,\
  ColumnName string,\
  ParentColumnName string,\
  ColumnOrder int,\
  ColumnType string,\
  IsSurrogateKeyFlg boolean,\
  IsBusinessKeyFlg boolean,\
  IsTimestampFlg boolean,\
  IsForeignKeyFlg boolean,\
  ForeignKeyTableName string\
  )\
  USING DELTA"
  spark.sql(sqlStr)

# COMMAND ----------

# MAGIC %md ####Populate metadata tables

# COMMAND ----------

def populateTableNames():
  spark.sql("truncate table ETL.TableNames")
  spark.sql("drop table if exists ETL.TableNames_STG")
  sqlStr="create  table ETL.TableNames_STG (\
  TableName_GLD string,\
  TableName_SLV string,\
  TableName_BRZ string,\
  SourcePath string,\
  TableType string,\
  ChangeType int,\
  WatermarkDate string,\
  LastId int\
  )\
  USING CSV\
  OPTIONS (header 'true')\
  LOCATION '/mnt/mda/source/Metadata/Metadata_Tables.csv'"
  spark.sql(sqlStr)      
  spark.sql("INSERT INTO ETL.TableNames SELECT TableName_GLD,TableName_SLV,TableName_BRZ,SourcePath,TableType,ChangeType,\
    CAST(WatermarkDate AS timestamp),LastId FROM ETL.TableNames_STG  WHERE TableName_BRZ is not null")
  return

def populateTableFields():
  spark.sql("truncate table ETL.TableFields")
  spark.sql("drop table if exists ETL.TableFields_STG")
  sqlStr="create  table ETL.TableFields_STG (\
  SchemaName string,\
  TableName string,\
  ColumnName string,\
  ParentColumnName string,\
  ColumnOrder int,\
  ColumnType string,\
  IsSurrogateKeyFlg int,\
  IsBusinessKeyFlg int,\
  IsTimestampFlg int,\
  IsForeignKeyFlg int,\
  ForeignKeyTableName string\
  )\
  USING CSV\
  OPTIONS (header 'true')\
  LOCATION '/mnt/mda/source/Metadata/Metadata_Columns.csv'"
  spark.sql(sqlStr)           
  spark.sql("insert into ETL.TableFields  select SchemaName,TableName,ColumnName,ParentColumnName,ColumnOrder,ColumnType,IsSurrogateKeyFlg,IsBusinessKeyFlg,IsTimestampFlg,\
     IsForeignKeyFlg ,ForeignKeyTableName from ETL.TableFields_STG WHERE TableName is not null ORDER BY TableName,ColumnOrder")
  return

# COMMAND ----------

# MAGIC %md ####Create DWH tables

# COMMAND ----------

def generateCreateStr(tableNm,tableSchema):
     dropCmdStr="DROP TABLE IF EXISTS {1}.{0}".format(tableNm,tableSchema) 
     createCmdStr="CREATE TABLE {1}.{0} ( ".format(tableNm,tableSchema)
     dfCols=spark.sql("SELECT ColumnName,ColumnType FROM ETL.TableFields WHERE TableName='{0}' ORDER BY ColumnOrder".format(tableNm))
     colsArr=dfCols.collect()
     colInd=0 
     for col in colsArr:
        if colInd==0:
          createCmdStr+="{0} {1} ".format(col.ColumnName,col.ColumnType)
        else:
           createCmdStr+=",{0} {1} ".format(col.ColumnName,col.ColumnType)
        colInd+=1
     createCmdStr+=") USING DELTA "
     return (dropCmdStr,createCmdStr)

def createDWHTables(SchemaName):
  if SchemaName=="BRONZE":
    TableColNm="TableName_BRZ"
  else:
    TableColNm="TableName_SLV"
  dfTabs=spark.sql("SELECT {0} FROM ETL.TableNames".format(TableColNm))
  tabLst=dfTabs.collect()
  for row in tabLst:
    dropBrzStr,createBrzStr=generateCreateStr(row[TableColNm],SchemaName)
    print(row[TableColNm])
    spark.sql(dropBrzStr)
    spark.sql(createBrzStr)
  return        

# COMMAND ----------

# MAGIC %md ####Utility functions

# COMMAND ----------

def getWatermarkData(TableName):
  sqlStrWtrm=("SELECT WatermarkDate,LastId FROM ETL.TableNames where TableName_SLV='{0}'").format(TableName)
  dfWM=spark.sql(sqlStrWtrm)
  return (dfWM.first())  

def updateWatermarkData(TableName,TableType):
  if TableType=='D':
    surrKeyName=getKeyFieldName(TableName,"IsSurrogateKeyFlg") 
    sqlMaxVal="SELECT MAX({1}) AS MaxID FROM SILVER.{0} ".format(TableName,surrKeyName)
    maxID=spark.sql(sqlMaxVal).first()['MaxID']
#   print(maxID)
    if maxID is not None:
      sqlStrWtrm=("UPDATE ETL.TableNames SET WatermarkDate =current_timestamp(),LastId={0} WHERE TableName_SLV='{1}'").format(maxID,TableName)
    else:
      print ("Table {0} has no rows".format(TableName))  
#     print (sqlStrWtrm)
  else:
      sqlStrWtrm=("UPDATE ETL.TableNames SET WatermarkDate =current_timestamp() WHERE TableName_SLV='{0}'").format(TableName)
  dfWM=spark.sql(sqlStrWtrm)
  return 

def getKeyFieldName(tableName,keyType):
    sqlKeys="SELECT ColumnName FROM ETL.TableFields WHERE TableName='{0}' and {1}=true".format(tableName,keyType)
    return spark.sql(sqlKeys).first()['ColumnName']

  
def getFieldList(SchemaName,TableName,includeForeignKeyFlg=False,tableAlias="",replaceForeignKeys=False):
  if includeForeignKeyFlg ==False:
    fkCondition="IsForeignKeyFlg=false"
  else:
    fkCondition="1=1"
  sqlKeys="SELECT ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND {2} ORDER BY ColumnOrder".format(SchemaName,TableName,fkCondition)
  rdd=spark.sql(sqlKeys).rdd.map(lambda x:','+tableAlias+x['ColumnName'])
  colLstStr=rdd.reduce(lambda x,y:x+y).replace(",","",1)
  joinAliasNum=1
  if replaceForeignKeys==True:
    sqlKeys="SELECT ColumnName,ForeignKeyTableName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsForeignKeyFlg=True ORDER BY ColumnOrder".format(SchemaName,TableName)
    keyRows=spark.sql(sqlKeys).collect()
    for keyRow in keyRows:
      primaryColumn,fkTable=keyRow
      surrKeyName=getKeyFieldName(fkTable,"IsSurrogateKeyFlg")
      surrKeyExp="T{0}.{1}".format(joinAliasNum,surrKeyName)
      refKeyExp="{0}{1}".format(tableAlias,primaryColumn)
#       print (refKeyExp,surrKeyExp)
      colLstStr=colLstStr.replace(refKeyExp,surrKeyExp)
      joinAliasNum+=1
#   print(colLstStr)   
  return colLstStr
 
def getBronzeTableName(tableName):
  sqlStrWtrm=("SELECT TableName_BRZ FROM ETL.TableNames where TableName_SLV='{0}'").format(tableName)
  dfWM=spark.sql(sqlStrWtrm)
  return (dfWM.first()['TableName_BRZ']) 

def getRowCounts():
  sqlCmd="SELECT TableName_BRZ,TableName_SLV FROM ETL.TableNames "
  dfRows=spark.sql(sqlCmd).collect()
  for row in dfRows:
    rowCnt_BRZ=spark.table("BRONZE.{0}".format(row['TableName_BRZ'])).count()
    rowCnt_SLV=spark.table("SILVER.{0}".format(row['TableName_SLV'])).count()
    print ("Row counts- {0}:{1}, {2}:{3}".format(row['TableName_BRZ'],rowCnt_BRZ,row['TableName_SLV'],rowCnt_SLV))                       
  return
  


# COMMAND ----------

# MAGIC %md ####BRONZE level ingestions

# COMMAND ----------

def ingestBronzeTables(truncateFirstFlg):
  df=spark.sql("Select TableName_BRZ,SourcePath from ETL.TableNames")
  tbsArr=df.collect()
  for tableItem in tbsArr:
      sourcePath="{0}{1}.csv".format(tableItem.SourcePath,tableItem.TableName_BRZ)
      df=spark.read.csv(sourcePath,header=True)
      df.createOrReplaceTempView("TempSource")
      fldLstStr=getFieldList("BRONZE",tableItem.TableName_BRZ,False,"",False)
      sqlStr="INSERT INTO BRONZE.{0} SELECT {1} FROM TempSource".format(tableItem.TableName_BRZ,fldLstStr)
      print("Ingestion for table {0} is complete".format(tableItem.TableName_BRZ))
#       print (sqlStr)
      if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE BRONZE.{0}".format(tableItem.TableName_BRZ))
      spark.sql(sqlStr)
  print(df.count())
  return  


# COMMAND ----------

# MAGIC %md ####SILVER level ingestions

# COMMAND ----------

# def getMatchSQLCmd(lastId,dimTableName,brzTableName,tmspFld,lastUpdDt,joinCond,matchFlg):
#   if matchFlg==True:
#     matchCond="not null"
#   else: 
#     matchCond= "null" 
#   return (("SELECT (ROW_NUMBER() OVER (ORDER BY S.{3})+{0}) AS DimSurrKey,S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate\
#  FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} where T.{3} is {6} and S.{3}>='{4}'").format(lastId,dimTableName,brzTableName,tmspFld,lastUpdDt,joinCond,matchCond))


  


# fkStructType=StructType([StructField("primFieldName",StringType(),True),
#                      StructField("fnKeyTableName",StringType(),True),
#                      StructField("fnSurrKey",StringType(),True),
#                      StructField("fnBussKeyName",StringType(),True)])




def getJoinConditions(schemaName,tableName):
  sqlCmd="SELECT ColumnName,ParentColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' and IsBusinessKeyFlg=true".format(schemaName,tableName)
  df=spark.sql(sqlCmd)
  if df.count()>0:
      rddMap=df.rdd.map(lambda x:" AND S.{0}=T.{1} ".format(x['ParentColumnName'],x['ColumnName']))
      joinCond=rddMap.reduce(lambda x,y:x+y).replace("AND","",1)  
  return joinCond

def getKeyFieldsList(schemaName,tableName,keyType):
    sqlKeys="SELECT ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' and {2}=true".format(schemaName,tableName,keyType)
    df=spark.sql(sqlKeys)
    if df.count()>0:
      rddMap=df.rdd.map(lambda x:','+x['ColumnName'])
      colLstStr=rddMap.reduce(lambda x,y:x+y)
    else:
      colLstStr=""
    return colLstStr

def getFnSkList(schemaName,tableName):
    fnKeysArr=getForeignSurrKeyMetadata("SILVER",tableName)
    fnSurrKeyLst=""
    if len(fnKeysArr)>0:
      
      for fnSet in fnKeysArr:
        fnSurrKeyLst+=(','+fnSet[2])
    return fnSurrKeyLst
  
def getForeignSurrKeyMetadata(schemaName,tableName):
  sqlKeys="SELECT ColumnName,ParentColumnName,ForeignKeyTableName FROM ETL.TableFields WHERE SchemaName='{1}' AND TableName='{0}' and IsForeignKeyFlg=true".format(tableName,schemaName)
#   fkKeyArr =getKeyFieldsArr(schemaName,tableName,"IsForeignKeyFlg")
  fkKeyArr =spark.sql(sqlKeys).collect()
  fkArr=[]
  for fk in fkKeyArr:
    primFieldName,primFieldNameRef,fnKeyTableName=fk
    fnSurrKey=getKeyFieldName(fnKeyTableName,"IsSurrogateKeyFlg") 
    fnBussKeyName=getKeyFieldName(fnKeyTableName,"IsBusinessKeyFlg")
    fnTimestampName=getKeyFieldName(fnKeyTableName,"IsTimestampFlg")
#     print(primFieldName,fnKeyTableName,fnSurrKey,fnBussKeyName)
    fkArr.append([primFieldName,fnKeyTableName,fnSurrKey,fnBussKeyName,primFieldNameRef])
  return fkArr

def ingestDimTables(tableArr,truncateFirstFlg):
  for tableItem in tableArr:
     sqlInsertStr,sqlUpdateStr=getUpdInsCmds(tableItem.TableName_SLV,'D')
     if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE SILVER.{0}".format(tableItem.TableName_SLV))
     else:   
        spark.sql(sqlUpdateStr)
     print(sqlInsertStr) 
#     print("Ingesting into table {0} is complete".format(tableItem.TableName_SLV))
     spark.sql(sqlInsertStr) 
#      print(sqlUpdateStr)
  return  


def getDimMergeCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond):
  return (("MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
  WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp()").format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond))


def getUpdInsCmds(tableName_SLV,tableType):
  tableName_BRZ=getBronzeTableName(tableName_SLV)          #Get BRONZE table name   
#   print (tableName_BRZ,tableName_SLV)
  sourceTargetJoinCond=getJoinConditions('SILVER',tableName_SLV)    #Get join conditions
#   print (sourceTargetJoinCond)
  tmspFld=getKeyFieldName(tableName_SLV,"IsTimestampFlg") #Get timestamp field name
  lastUpdDt,lastId=getWatermarkData(tableName_SLV)        #Get watermark data    
  if tableType=='D':
    fldLstStr=getFieldList("SILVER",tableName_SLV,True,"")
    updateCmd=getDimMergeCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond)
    insertCmd=getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldLstStr)
  else:
    fldLstStr =getFieldList("SILVER",tableName_SLV,True,"S.",True)
    updateCmd=getFactMergeCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond)
    insertCmd=getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldLstStr)
  updateWatermarkData(tableName_SLV,tableType) 
  return (insertCmd,updateCmd)

def getFactMergeCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond):
  sqlCmd="SELECT ParentColumnName,ColumnName FROM ETL.TableFields WHERE SchemaName='{0}' AND TableName='{1}' AND IsBusinessKeyFlg=False AND IsTimestampFlg=False AND IsForeignKeyFlg=False ORDER BY ColumnOrder".format("SILVER",tableName_SLV)
  df=spark.sql(sqlCmd)
#   if df.count()>0:
  rddMap=df.rdd.map(lambda x:",T.{0}=S.{1}".format(x['ColumnName'],x['ParentColumnName']))
  fldUpdStr=rddMap.reduce(lambda x,y:x+y).replace(',','',1)
  fldUpdStr+=",{0}=current_timestamp() ".format(tmspFld)
  return (("MERGE INTO SILVER.{0} AS T USING BRONZE.{1} AS S ON {4} AND S.{3}>='{2}' \
  WHEN MATCHED THEN UPDATE SET {5}").format(tableName_SLV,tableName_BRZ,lastUpdDt,tmspFld,joinCond,fldUpdStr))

def getDimInsertCmd(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr):
  surrKeyName=getKeyFieldName(tableName_SLV,"IsSurrogateKeyFlg")
  fnSKLst=getFnSkList("SILVER",tableName_SLV)
  return (("INSERT INTO SILVER.{1} SELECT {6} FROM (SELECT (ROW_NUMBER() OVER (ORDER BY S.{3})+{0}) AS {7},S.*,current_timestamp() as ValidFromDate,\
  cast('9999-12-31' as timestamp) AS ValidToDate {8} FROM BRONZE.{2} AS S LEFT JOIN SILVER.{1} AS T ON {5} where T.{3} IS NULL AND S.{3}>='{4}')")
   .format(lastId,tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,joinCond,fldListStr,surrKeyName,fnSKLst))

def getFactInsertCmd(tableName_SLV,tableName_BRZ,tmspFld,lastUpdDt,sourceTargetJoinCond,fldListStr):
    fnSurrKeyLst=""
    joinStr=""
    aliasNum=1
    fnKeysArr=getForeignSurrKeyMetadata("SILVER",tableName_SLV)
    for fnSet in fnKeysArr:
      joinStr+= " LEFT JOIN {4}.{1} AS T{5} ON S.{3}=T{5}.{2} AND T{5}.ValidToDate=CAST('9999-12-31' as timestamp)".format(fnSet[0],fnSet[1],fnSet[3],fnSet[4],"SILVER",aliasNum)
      aliasNum+=1
    insertCmd="INSERT INTO SILVER.{1} SELECT {2} FROM BRONZE.{0} AS S LEFT JOIN SILVER.{1} AS T ON {3}  {6} WHERE T.{4} IS NULL AND S.{4}>='{5}' ".format(tableName_BRZ,tableName_SLV,fldListStr,sourceTargetJoinCond,tmspFld,lastUpdDt,joinStr)
    print (insertCmd)
#   print("Ingestiion for {0} is complete".format(tableItem.TableName_BRZ))
    return (insertCmd)   #Add update statement



def ingestFactTables(tableArr,truncateFirstFlg):
  for tableItem in tableArr:
     sqlInsertStr,sqlUpdateStr=getUpdInsCmds(tableItem.TableName_SLV,'F')
#      sqlInsertStr,sqlUpdateStr=getFactInsertCmd("SILVER",tableItem.TableName_SLV)
     if truncateFirstFlg ==True:
        spark.sql("TRUNCATE TABLE SILVER.{0}".format(tableItem.TableName_SLV))
     else:   
        spark.sql(sqlUpdateStr)
     print(sqlUpdateStr) 
     print(sqlInsertStr) 
     spark.sql(sqlInsertStr) 
#      print(sqlUpdateStr)
  return  

def ingestSilverTables(truncateFirstFlg):
#   df=spark.sql("Select TableName_SLV FROM ETL.TableNames WHERE TableType='D'")
#   tbsArr=df.collect()
#   ingestDimTables(tbsArr,truncateFirstFlg)
  df=spark.sql("Select TableName_SLV FROM ETL.TableNames WHERE TableType='F'")
  tbsArr=df.collect()
  ingestFactTables(tbsArr,truncateFirstFlg)
  return


# COMMAND ----------

def getDimResults_Old(DimTableName):
  joinCond=getJoinConditions(DimTableName)               #Get join conditions
  tmspFld=getTimestampField(DimTableName)                #Get timestamp field name
  lastUpdDt,lastId=getWatermarkData(DimTableName)        #Get watermark data    

  sqlStrMatch=("SELECT monotonically_increasing_id() AS CustomerKey,S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is not null and S.{4}>='{2}'").format(DimTableName,keyFieldName,lastUpdDt,lastId,tmspFld)
  print (sqlStrMatch)
#   dfM=spark.sql(sqlStrMatch)
  
  updStrMatch=("MERGE INTO {0} AS T USING {0}_RAW AS S ON "+joinCond+" AND S.{3}>='{2}'  WHEN MATCHED THEN UPDATE SET ValidToDate=current_timestamp() ").format(DimTableName,keyFieldName,lastUpdDt,tmspFld)
  print (updStrMatch)
#   dfM=spark.sql(updStrMatch)
  
#   sqlStrNoMatch=("SELECT (ROW_NUMBER() OVER (ORDER BY S.{1})+{2}) AS CustomerKey, S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate  FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is null").format(TableName,keyFieldName,lastId)
  sqlStrNoMatch=("SELECT monotonically_increasing_id() AS CustomerKey, S.*,current_timestamp() as ValidFromDate,cast('9999-12-31' as timestamp) AS ValidToDate  FROM {0}_RAW AS S LEFT JOIN {0} AS T ON "+joinCond+" where T.{1} is null").format(DimTableName,keyFieldName,lastId)  
#   dfNM=spark.sql(sqlStrNoMatch)
  
  return (sqlStrNoMatch,sqlStrMatch,updStrMatch)
  

# COMMAND ----------

